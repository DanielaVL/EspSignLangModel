{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de los estimadores de pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Función para dibujar puntos de la pose del cuerpo\n",
    "def draw_pose_landmarks(frame, landmarks, COLOR):\n",
    "    height, width, _ = frame.shape\n",
    "    for point in landmarks.landmark:\n",
    "        x, y = int(point.x * width), int(point.y * height)\n",
    "        cv2.circle(frame, (x, y), 5, COLOR, -1)\n",
    "\n",
    "# Función para dibujar puntos y líneas de las manos\n",
    "def draw_hand_landmarks_with_square(frame, landmarks, COLOR):\n",
    "    height, width, _ = frame.shape\n",
    "    x_min, y_min, x_max, y_max = width, height, 0, 0\n",
    "    \n",
    "    # Dibujar puntos y líneas de las manos\n",
    "    for i, point in enumerate(landmarks.landmark):\n",
    "        x, y = int(point.x * width), int(point.y * height)\n",
    "        color = (0, 255, 0) if i % 4 == 0 else (0, 0, 255)\n",
    "        cv2.circle(frame, (x, y), int(height * 0.01), color, -1)\n",
    "        # Actualizar los límites del cuadrado\n",
    "        x_min = min(x_min, x)\n",
    "        y_min = min(y_min, y)\n",
    "        x_max = max(x_max, x)\n",
    "        y_max = max(y_max, y)\n",
    "    \n",
    "    connections = [[0, 1], [1, 2], [2, 3], [3, 4], [0, 5], [5, 6], [6, 7], [7, 8], [0, 9], [9, 10], [10, 11], [11, 12], [0, 13], [13, 14], [14, 15], [15, 16], [0, 17], [17, 18], [18, 19], [19, 20]]\n",
    "    for connection in connections:\n",
    "        cv2.line(frame, (int(landmarks.landmark[connection[0]].x * width), int(landmarks.landmark[connection[0]].y * height)),\n",
    "                 (int(landmarks.landmark[connection[1]].x * width), int(landmarks.landmark[connection[1]].y * height)), COLOR, 2)\n",
    "    cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (255, 0, 255), 2)\n",
    "\n",
    "# Función para dibujar cuadro de la cara\n",
    "def draw_face_detection(frame, detections, COLOR):\n",
    "    height, width, _ = frame.shape\n",
    "    for detection in detections:\n",
    "        bboxC = detection.location_data.relative_bounding_box\n",
    "        ih, iw, _ = frame.shape\n",
    "        bbox = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "        cv2.rectangle(frame, bbox, COLOR, 2)\n",
    "\n",
    "# Inicializar los modelos de MediaPipe\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_face = mp.solutions.face_detection\n",
    "\n",
    "pose = mp_pose.Pose()\n",
    "hands = mp_hands.Hands()\n",
    "face_detection = mp_face.FaceDetection()\n",
    "\n",
    "# Constantes para colores\n",
    "RED = (0, 0, 255)\n",
    "GREEN = (0, 255, 0)\n",
    "BLUE = (255, 0, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Captura en tiempo real de la pose de una seña"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'draw_hand_landmarks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hands_results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmarks \u001b[38;5;129;01min\u001b[39;00m hands_results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n\u001b[1;32m---> 35\u001b[0m         \u001b[43mdraw_hand_landmarks\u001b[49m(black_frame, landmarks, BLUE)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Dibujar puntos y cuadro de la cara en la imagen original\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# if face_results.detections:\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m#     draw_face_detection(frame, face_results.detections)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPose Detection\u001b[39m\u001b[38;5;124m'\u001b[39m, black_frame)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'draw_hand_landmarks' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error al abrir la cámara\")\n",
    "else:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error al leer el frame\")\n",
    "            break\n",
    "\n",
    "        # height, width, _ = frame.shape\n",
    "\n",
    "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Detección de la pose del cuerpo\n",
    "        pose_results = pose.process(img)\n",
    "\n",
    "        # Detección de las manos\n",
    "        hands_results = hands.process(img)\n",
    "\n",
    "        # Detección de la cara\n",
    "        face_results = face_detection.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Crear una imagen negra del mismo tamaño que el frame\n",
    "        black_frame = np.zeros_like(frame)\n",
    "\n",
    "        # Dibujar puntos y líneas para la pose del cuerpo en la imagen negra\n",
    "        # if pose_results.pose_landmarks:\n",
    "        #     draw_pose_landmarks(black_frame, pose_results.pose_landmarks)\n",
    "\n",
    "        # Dibujar círculos y líneas para las manos en la imagen negra\n",
    "        if hands_results.multi_hand_landmarks:\n",
    "            for landmarks in hands_results.multi_hand_landmarks:\n",
    "                draw_hand_landmarks(black_frame, landmarks, BLUE)\n",
    "\n",
    "        # Dibujar puntos y cuadro de la cara en la imagen original\n",
    "        # if face_results.detections:\n",
    "        #     draw_face_detection(frame, face_results.detections)\n",
    "\n",
    "        cv2.imshow('Pose Detection', black_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso del modelo en tiempo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import cv2\n",
    "\n",
    "\n",
    "model = load_model('model.h5')\n",
    "\n",
    "class_indices = json.load(open('class_indices.json'))\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    img = cv2.resize(frame, (224, 224))\n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "    prediction = model.predict(img)\n",
    "    predicted_class = list(class_indices.keys())[np.argmax(prediction)]\n",
    "    cv2.putText(frame, predicted_class, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "model = load_model('model.h5')\n",
    "\n",
    "class_indices = json.load(open('class_indices.json'))\n",
    "\n",
    "def predict_image(model, img_path, class_indices):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "    prediction = model.predict(img)\n",
    "    predicted_class = list(class_indices.keys())[np.argmax(prediction)]\n",
    "    return predicted_class\n",
    "\n",
    "img_path = \"./classes_original/M/DSC01254.JPG\"\n",
    "predicted_class = predict_image(model, img_path, class_indices)\n",
    "print(predicted_class)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
