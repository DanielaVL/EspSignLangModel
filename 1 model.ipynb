{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de clasificación de imágenes de lengua de señas en Español"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de muestras por clase antes del balanceo:\n",
      "R: 131 muestras\n",
      "A: 147 muestras\n",
      "O: 134 muestras\n",
      "Q: 130 muestras\n",
      "U: 135 muestras\n",
      "T: 126 muestras\n",
      "F: 129 muestras\n",
      "M: 130 muestras\n",
      "I: 135 muestras\n",
      "G: 129 muestras\n",
      "D: 132 muestras\n",
      "S: 135 muestras\n",
      "C: 140 muestras\n",
      "N: 131 muestras\n",
      "K: 134 muestras\n",
      "E: 134 muestras\n",
      "P: 126 muestras\n",
      "L: 132 muestras\n",
      "B: 102 muestras\n",
      "\n",
      "Número de muestras por clase después del balanceo:\n",
      "R: 147 muestras\n",
      "A: 147 muestras\n",
      "O: 147 muestras\n",
      "Q: 147 muestras\n",
      "U: 147 muestras\n",
      "T: 147 muestras\n",
      "F: 147 muestras\n",
      "M: 147 muestras\n",
      "I: 147 muestras\n",
      "G: 147 muestras\n",
      "D: 147 muestras\n",
      "S: 147 muestras\n",
      "C: 147 muestras\n",
      "N: 147 muestras\n",
      "K: 147 muestras\n",
      "E: 147 muestras\n",
      "P: 147 muestras\n",
      "L: 147 muestras\n",
      "B: 147 muestras\n",
      "Epoch 1/5\n",
      "52/52 [==============================] - 468s 9s/step - loss: 5.2908 - accuracy: 0.0499 - val_loss: 2.9451 - val_accuracy: 0.0429\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 458s 9s/step - loss: 2.9443 - accuracy: 0.0456 - val_loss: 2.9456 - val_accuracy: 0.0411\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 452s 9s/step - loss: 2.9441 - accuracy: 0.0596 - val_loss: 2.9461 - val_accuracy: 0.0429\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 450s 9s/step - loss: 2.9440 - accuracy: 0.0463 - val_loss: 2.9464 - val_accuracy: 0.0429\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 451s 9s/step - loss: 2.9437 - accuracy: 0.0578 - val_loss: 2.9469 - val_accuracy: 0.0429\n",
      "18/18 [==============================] - 25s 1s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        24\n",
      "           1       0.00      0.00      0.00        36\n",
      "           2       0.00      0.00      0.00        29\n",
      "           3       0.00      0.00      0.00        40\n",
      "           4       0.00      0.00      0.00        27\n",
      "           5       0.00      0.00      0.00        29\n",
      "           6       0.00      0.00      0.00        24\n",
      "           7       0.00      0.00      0.00        29\n",
      "           8       0.00      0.00      0.00        29\n",
      "           9       0.00      0.00      0.00        36\n",
      "          10       0.00      0.00      0.00        32\n",
      "          11       0.00      0.00      0.00        25\n",
      "          12       0.00      0.00      0.00        28\n",
      "          13       0.00      0.00      0.00        27\n",
      "          14       0.00      0.00      0.00        32\n",
      "          15       0.00      0.00      0.00        26\n",
      "          16       0.00      0.00      0.00        29\n",
      "          17       0.00      0.00      0.00        31\n",
      "          18       0.05      1.00      0.09        26\n",
      "\n",
      "    accuracy                           0.05       559\n",
      "   macro avg       0.00      0.05      0.00       559\n",
      "weighted avg       0.00      0.05      0.00       559\n",
      "\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 24]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 36]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 40]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 27]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 24]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 36]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 32]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 25]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 28]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 27]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 32]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 26]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 31]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 26]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kuro\\Documents\\projects\\EspSignLangModel\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Kuro\\Documents\\projects\\EspSignLangModel\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Kuro\\Documents\\projects\\EspSignLangModel\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Kuro\\Documents\\projects\\EspSignLangModel\\env\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "\n",
    "data_path = \"./classes_poses_cut/\"\n",
    "\n",
    "def load_images(path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    classes = os.listdir(path)\n",
    "    class_count = len(classes)\n",
    "\n",
    "    for class_name in classes:\n",
    "        class_path = os.path.join(path, class_name)\n",
    "        for file_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, file_name)\n",
    "\n",
    "            img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
    "            img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "            img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "\n",
    "            images.append(img)\n",
    "            labels.append(class_name)\n",
    "\n",
    "    return np.array(images), np.array(labels), class_count\n",
    "\n",
    "X, y, class_count = load_images(data_path)\n",
    "\n",
    "# Imprimir el número de muestras por clase antes del balanceo\n",
    "print(\"Número de muestras por clase antes del balanceo:\")\n",
    "for class_name in set(y):\n",
    "    print(f\"{class_name}: {np.sum(y == class_name)} muestras\")\n",
    "\n",
    "# Aplicar oversampling a las clases menos representadas\n",
    "oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X.reshape(-1, 224 * 224 * 3), y)\n",
    "\n",
    "X_resampled = X_resampled.reshape(-1, 224, 224, 3)\n",
    "\n",
    "# Imprimir el número de muestras por clase después del balanceo\n",
    "print(\"\\nNúmero de muestras por clase después del balanceo:\")\n",
    "for class_name in set(y_resampled):\n",
    "    print(f\"{class_name}: {np.sum(y_resampled == class_name)} muestras\")\n",
    "\n",
    "# Codificar las etiquetas de clase\n",
    "label_encoder = LabelEncoder()\n",
    "y_resampled_encoded = label_encoder.fit_transform(y_resampled)\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento, validación y prueba\n",
    "X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(X_resampled, y_resampled_encoded, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train_encoded, y_val_encoded = train_test_split(X_train, y_train_encoded, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "# Crear un generador de imágenes con data augmentation para el entrenamiento\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Ajustar el generador de datos de entrenamiento\n",
    "train_datagen.fit(X_train)\n",
    "\n",
    "# Definir el modelo\n",
    "def create_model(kernel_size, num_layers, learning_rate):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(64, (kernel_size, kernel_size), activation='relu', input_shape=(224, 224, 3)))\n",
    "    for _ in range(num_layers - 1):\n",
    "        model.add(tf.keras.layers.Conv2D(64, (kernel_size, kernel_size), activation='relu'))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(class_count, activation='softmax'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Entrenar el modelo con data augmentation\n",
    "best_params = {'kernel_size': 5, 'num_layers': 3, 'learning_rate': 0.001}\n",
    "final_model = create_model(**best_params)\n",
    "\n",
    "final_model.fit(train_datagen.flow(X_train, y_train_encoded, batch_size=32),\n",
    "                steps_per_epoch=len(X_train) // 32,\n",
    "                validation_data=(X_val, y_val_encoded),\n",
    "                epochs=5)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "y_pred = final_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_test_encoded, y_pred_classes))\n",
    "print(confusion_matrix(y_test_encoded, y_pred_classes))\n",
    "\n",
    "# Guardar el modelo\n",
    "final_model.save('model.h5')\n",
    "\n",
    "# Guardar los índices de clase en un archivo JSON\n",
    "class_indices = {class_name: i for i, class_name in enumerate(label_encoder.classes_)}\n",
    "with open('class_indices.json', 'w') as f:\n",
    "    json.dump(class_indices, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "import json\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_path = \"./classes_poses/\"\n",
    "\n",
    "def load_images(path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    classes = os.listdir(path)\n",
    "    class_count = len(classes)\n",
    "\n",
    "    for class_name in classes:\n",
    "        class_path = os.path.join(path, class_name)\n",
    "        for file_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, file_name)\n",
    "\n",
    "            img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
    "            img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "            img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "\n",
    "            images.append(img)\n",
    "            labels.append(class_name)\n",
    "\n",
    "    return np.array(images), np.array(labels), class_count\n",
    "\n",
    "X, y, class_count = load_images(data_path)\n",
    "\n",
    "# Imprimir el número de muestras por clase antes del balanceo\n",
    "print(\"Número de muestras por clase antes del balanceo:\")\n",
    "for class_name in set(y):\n",
    "    print(f\"{class_name}: {np.sum(y == class_name)} muestras\")\n",
    "\n",
    "# Aplicar oversampling a las clases menos representadas\n",
    "oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X.reshape(-1, 224 * 224 * 3), y)\n",
    "\n",
    "X_resampled = X_resampled.reshape(-1, 224, 224, 3)\n",
    "\n",
    "# Crear un generador de imágenes con data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Aplicar data augmentation al conjunto de datos resampleado\n",
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "\n",
    "for img, label in zip(X_resampled, y_resampled):\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    for _ in range(3):  # Aumentar el conjunto de datos tres veces\n",
    "        augmented_img = datagen.random_transform(img[0])\n",
    "        augmented_images.append(augmented_img)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "X_ = np.array(augmented_images)\n",
    "y_ = np.array(augmented_labels)\n",
    "\n",
    "\n",
    "# Imprimir el número de muestras por clase después del balanceo\n",
    "print(\"\\nNúmero de muestras por clase después del balanceo:\")\n",
    "for class_name in set(y_):\n",
    "    print(f\"{class_name}: {np.sum(y_ == class_name)} muestras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partición de datos \n",
    "\n",
    "eliminar ruido mejorando la forma del augmentation,\n",
    "borrar imagenes aug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X_, y_, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "num_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación de variables categóricas a numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class_indices = {}\n",
    "for i, class_name in enumerate(np.unique(y_train)):\n",
    "    class_indices[class_name] = i\n",
    "with open('class_indices.json', 'w') as f:\n",
    "    json.dump(class_indices, f)\n",
    "\n",
    "y_train = np.vectorize(class_indices.get)(y_train)\n",
    "y_val = np.vectorize(class_indices.get)(y_val)\n",
    "y_test = np.vectorize(class_indices.get)(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurar el modelo de clasificación con CNN\n",
    "\n",
    "como se hace con un dataloader la aug en linea \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(kernel_size, num_layers, batch_size, learning_rate):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(64, (kernel_size, kernel_size), activation='relu', input_shape=(224, 224, 3)))\n",
    "    for _ in range(num_layers - 1):\n",
    "        model.add(tf.keras.layers.Conv2D(64, (kernel_size, kernel_size), activation='relu'))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "134/134 [==============================] - 634s 5s/step - loss: 6.0044 - accuracy: 0.0513 - val_loss: 2.9449 - val_accuracy: 0.0448\n",
      "Epoch 2/10\n",
      "134/134 [==============================] - 624s 5s/step - loss: 2.9445 - accuracy: 0.0497 - val_loss: 2.9454 - val_accuracy: 0.0459\n",
      "Epoch 3/10\n",
      "134/134 [==============================] - 624s 5s/step - loss: 2.9439 - accuracy: 0.0506 - val_loss: 2.9424 - val_accuracy: 0.0557\n",
      "Epoch 4/10\n",
      "134/134 [==============================] - 627s 5s/step - loss: 2.7858 - accuracy: 0.2036 - val_loss: 2.9965 - val_accuracy: 0.0798\n",
      "Epoch 5/10\n",
      "134/134 [==============================] - 652s 5s/step - loss: 0.3690 - accuracy: 0.9117 - val_loss: 6.4031 - val_accuracy: 0.0689\n",
      "Epoch 6/10\n",
      "134/134 [==============================] - 661s 5s/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 8.6381 - val_accuracy: 0.0732\n",
      "Epoch 7/10\n",
      "134/134 [==============================] - 659s 5s/step - loss: 7.8432e-05 - accuracy: 1.0000 - val_loss: 9.1718 - val_accuracy: 0.0743\n",
      "Epoch 8/10\n",
      "134/134 [==============================] - 652s 5s/step - loss: 4.5902e-05 - accuracy: 1.0000 - val_loss: 9.5434 - val_accuracy: 0.0743\n",
      "Epoch 9/10\n",
      "134/134 [==============================] - 655s 5s/step - loss: 3.1199e-05 - accuracy: 1.0000 - val_loss: 9.8788 - val_accuracy: 0.0754\n",
      "Epoch 10/10\n",
      "134/134 [==============================] - 662s 5s/step - loss: 2.2523e-05 - accuracy: 1.0000 - val_loss: 10.1660 - val_accuracy: 0.0754\n",
      "29/29 [==============================] - 23s 776ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.05      0.04      0.04        53\n",
      "           1       0.00      0.00      0.00        49\n",
      "           2       0.06      0.08      0.07        50\n",
      "           3       0.11      0.19      0.13        43\n",
      "           4       0.07      0.03      0.04        60\n",
      "           5       0.08      0.09      0.09        54\n",
      "           6       0.13      0.11      0.12        46\n",
      "           7       0.10      0.14      0.12        44\n",
      "           8       0.17      0.16      0.16        51\n",
      "           9       0.09      0.08      0.08        49\n",
      "          10       0.07      0.04      0.05        45\n",
      "          11       0.05      0.04      0.04        51\n",
      "          12       0.08      0.15      0.10        41\n",
      "          13       0.06      0.09      0.07        47\n",
      "          14       0.08      0.07      0.08        43\n",
      "          15       0.14      0.15      0.14        48\n",
      "          16       0.06      0.05      0.05        42\n",
      "          17       0.07      0.06      0.06        50\n",
      "          18       0.13      0.10      0.11        49\n",
      "\n",
      "    accuracy                           0.09       915\n",
      "   macro avg       0.08      0.09      0.08       915\n",
      "weighted avg       0.08      0.09      0.08       915\n",
      "\n",
      "[[ 2  2  1  4  2  6  4 10  0  0  4  3  4  2  5  2  0  1  1]\n",
      " [ 1  0  4  5  4  5  2  1  3  4  2  0  3  1  3  4  2  4  1]\n",
      " [ 3  2  4  5  0  2  2  4  3  3  1  2  2  4  3  2  3  4  1]\n",
      " [ 2  1  1  8  0  4  1  1  3  3  2  2  5  2  1  0  2  3  2]\n",
      " [ 4  4  6  8  2  2  2  3  2  3  0  2  3  5  2  4  3  2  3]\n",
      " [ 1  1  3  1  2  5  3  1  3  4  3  3  5  4  4  2  3  3  3]\n",
      " [ 3  2  1  3  1  7  5  3  2  3  1  1  1  3  0  4  3  2  1]\n",
      " [ 0  2  1  2  2  3  1  6  5  2  1  1  4  4  3  1  2  2  2]\n",
      " [ 2  3  2  4  0  3  3  5  8  1  2  1  4  1  1  5  1  3  2]\n",
      " [ 5  1  3  3  0  4  1  2  2  4  2  1  8  3  0  2  2  1  5]\n",
      " [ 1  2  2  3  5  2  2  0  2  1  2  6  6  3  0  4  2  0  2]\n",
      " [ 1  5  2  5  1  1  1  5  6  3  1  2  3  6  1  1  0  2  5]\n",
      " [ 1  1  6  3  1  1  2  2  2  2  0  0  6  6  0  2  4  1  1]\n",
      " [ 4  1  6  3  2  3  0  1  0  2  0  5  4  4  2  2  2  4  2]\n",
      " [ 3  2  3  4  2  2  2  3  1  0  1  4  2  5  3  2  2  2  0]\n",
      " [ 3  0  2  4  3  2  1  5  0  3  2  3  2  4  2  7  2  3  0]\n",
      " [ 1  5  6  1  1  3  4  2  0  2  1  1  4  3  2  1  2  2  1]\n",
      " [ 3  1  7  2  1  4  2  4  4  2  2  2  5  1  2  2  1  3  2]\n",
      " [ 2  2  5  8  1  2  1  2  1  4  3  1  4  1  2  3  0  2  5]]\n"
     ]
    }
   ],
   "source": [
    "# param_grid = {\n",
    "#     'kernel_size': [3, 5], #!!\n",
    "#     'num_layers': [2, 3],\n",
    "#     'batch_size': [32, 64],\n",
    "#     'learning_rate': [0.001, 0.0001]\n",
    "# }\n",
    "# param_grid = {\n",
    "#     'kernel_size': [5],\n",
    "#     'num_layers': [3],\n",
    "#     'batch_size': [32],\n",
    "#     'learning_rate': [0.001]\n",
    "# }\n",
    "\n",
    "# model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "\n",
    "# grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# best_params = grid_result.best_params_\n",
    "\n",
    " \n",
    "best_params = {'batch_size': 32, 'kernel_size': 5, 'learning_rate': 0.001, 'num_layers': 3}\n",
    "final_model = create_model(**best_params)\n",
    "final_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
    "\n",
    "y_pred = final_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "print(confusion_matrix(y_test, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.save('model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detección de clase con una sola imágen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 372ms/step\n",
      "M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = load_model('model.h5')\n",
    "\n",
    "class_indices = json.load(open('class_indices.json'))\n",
    "\n",
    "def predict_image(model, img_path, class_indices):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "    prediction = model.predict(img)\n",
    "    predicted_class = list(class_indices.keys())[np.argmax(prediction)]\n",
    "    return predicted_class\n",
    "\n",
    "img_path = \"./classes_original/M/DSC01254.JPG\"\n",
    "predicted_class = predict_image(model, img_path, class_indices)\n",
    "print(predicted_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detección de clases en tiempo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quiero que me haga predicciones en tiempo real con la camara\n",
    "\n",
    "\n",
    "model = load_model('model.h5')\n",
    "\n",
    "class_indices = json.load(open('class_indices.json'))\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    img = cv2.resize(frame, (224, 224))\n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "    prediction = model.predict(img)\n",
    "    predicted_class = list(class_indices.keys())[np.argmax(prediction)]\n",
    "    cv2.putText(frame, predicted_class, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
