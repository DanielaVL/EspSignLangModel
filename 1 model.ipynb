{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de clasificación de imágenes de lengua de señas en Español"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "import json\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de muestras por clase antes del balanceo:\n",
      "K: 96 muestras\n",
      "L: 107 muestras\n",
      "G: 97 muestras\n",
      "E: 99 muestras\n",
      "D: 81 muestras\n",
      "I: 106 muestras\n",
      "N: 101 muestras\n",
      "M: 102 muestras\n",
      "O: 93 muestras\n",
      "F: 102 muestras\n",
      "Q: 85 muestras\n",
      "C: 61 muestras\n",
      "P: 90 muestras\n",
      "B: 91 muestras\n",
      "S: 98 muestras\n",
      "U: 100 muestras\n",
      "T: 93 muestras\n",
      "A: 98 muestras\n",
      "R: 77 muestras\n",
      "\n",
      "Número de muestras por clase después del balanceo:\n",
      "K: 321 muestras\n",
      "L: 321 muestras\n",
      "G: 321 muestras\n",
      "E: 321 muestras\n",
      "D: 321 muestras\n",
      "I: 321 muestras\n",
      "N: 321 muestras\n",
      "M: 321 muestras\n",
      "O: 321 muestras\n",
      "F: 321 muestras\n",
      "Q: 321 muestras\n",
      "C: 321 muestras\n",
      "P: 321 muestras\n",
      "B: 321 muestras\n",
      "S: 321 muestras\n",
      "U: 321 muestras\n",
      "T: 321 muestras\n",
      "A: 321 muestras\n",
      "R: 321 muestras\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_path = \"./classes_poses/\"\n",
    "\n",
    "def load_images(path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    classes = os.listdir(path)\n",
    "    class_count = len(classes)\n",
    "\n",
    "    for class_name in classes:\n",
    "        class_path = os.path.join(path, class_name)\n",
    "        for file_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, file_name)\n",
    "\n",
    "            img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
    "            img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "            img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "\n",
    "            images.append(img)\n",
    "            labels.append(class_name)\n",
    "\n",
    "    return np.array(images), np.array(labels), class_count\n",
    "\n",
    "X, y, class_count = load_images(data_path)\n",
    "\n",
    "# Imprimir el número de muestras por clase antes del balanceo\n",
    "print(\"Número de muestras por clase antes del balanceo:\")\n",
    "for class_name in set(y):\n",
    "    print(f\"{class_name}: {np.sum(y == class_name)} muestras\")\n",
    "\n",
    "# Aplicar oversampling a las clases menos representadas\n",
    "oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X.reshape(-1, 224 * 224 * 3), y)\n",
    "\n",
    "X_resampled = X_resampled.reshape(-1, 224, 224, 3)\n",
    "\n",
    "# Crear un generador de imágenes con data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Aplicar data augmentation al conjunto de datos resampleado\n",
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "\n",
    "for img, label in zip(X_resampled, y_resampled):\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    for _ in range(3):  # Aumentar el conjunto de datos tres veces\n",
    "        augmented_img = datagen.random_transform(img[0])\n",
    "        augmented_images.append(augmented_img)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "X_ = np.array(augmented_images)\n",
    "y_ = np.array(augmented_labels)\n",
    "\n",
    "\n",
    "# Imprimir el número de muestras por clase después del balanceo\n",
    "print(\"\\nNúmero de muestras por clase después del balanceo:\")\n",
    "for class_name in set(y_):\n",
    "    print(f\"{class_name}: {np.sum(y_ == class_name)} muestras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partición de datos \n",
    "\n",
    "eliminar ruido mejorando la forma del augmentation,\n",
    "borrar imagenes aug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X_, y_, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "num_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación de variables categóricas a numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class_indices = {}\n",
    "for i, class_name in enumerate(np.unique(y_train)):\n",
    "    class_indices[class_name] = i\n",
    "with open('class_indices.json', 'w') as f:\n",
    "    json.dump(class_indices, f)\n",
    "\n",
    "y_train = np.vectorize(class_indices.get)(y_train)\n",
    "y_val = np.vectorize(class_indices.get)(y_val)\n",
    "y_test = np.vectorize(class_indices.get)(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurar el modelo de clasificación con CNN\n",
    "\n",
    "como se hace con un dataloader la aug en linea \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(kernel_size, num_layers, batch_size, learning_rate):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(64, (kernel_size, kernel_size), activation='relu', input_shape=(224, 224, 3)))\n",
    "    for _ in range(num_layers - 1):\n",
    "        model.add(tf.keras.layers.Conv2D(64, (kernel_size, kernel_size), activation='relu'))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "134/134 [==============================] - 634s 5s/step - loss: 6.0044 - accuracy: 0.0513 - val_loss: 2.9449 - val_accuracy: 0.0448\n",
      "Epoch 2/10\n",
      "134/134 [==============================] - 624s 5s/step - loss: 2.9445 - accuracy: 0.0497 - val_loss: 2.9454 - val_accuracy: 0.0459\n",
      "Epoch 3/10\n",
      "134/134 [==============================] - 624s 5s/step - loss: 2.9439 - accuracy: 0.0506 - val_loss: 2.9424 - val_accuracy: 0.0557\n",
      "Epoch 4/10\n",
      "134/134 [==============================] - 627s 5s/step - loss: 2.7858 - accuracy: 0.2036 - val_loss: 2.9965 - val_accuracy: 0.0798\n",
      "Epoch 5/10\n",
      "134/134 [==============================] - 652s 5s/step - loss: 0.3690 - accuracy: 0.9117 - val_loss: 6.4031 - val_accuracy: 0.0689\n",
      "Epoch 6/10\n",
      "134/134 [==============================] - 661s 5s/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 8.6381 - val_accuracy: 0.0732\n",
      "Epoch 7/10\n",
      "134/134 [==============================] - 659s 5s/step - loss: 7.8432e-05 - accuracy: 1.0000 - val_loss: 9.1718 - val_accuracy: 0.0743\n",
      "Epoch 8/10\n",
      "134/134 [==============================] - 652s 5s/step - loss: 4.5902e-05 - accuracy: 1.0000 - val_loss: 9.5434 - val_accuracy: 0.0743\n",
      "Epoch 9/10\n",
      "134/134 [==============================] - 655s 5s/step - loss: 3.1199e-05 - accuracy: 1.0000 - val_loss: 9.8788 - val_accuracy: 0.0754\n",
      "Epoch 10/10\n",
      "134/134 [==============================] - 662s 5s/step - loss: 2.2523e-05 - accuracy: 1.0000 - val_loss: 10.1660 - val_accuracy: 0.0754\n",
      "29/29 [==============================] - 23s 776ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.05      0.04      0.04        53\n",
      "           1       0.00      0.00      0.00        49\n",
      "           2       0.06      0.08      0.07        50\n",
      "           3       0.11      0.19      0.13        43\n",
      "           4       0.07      0.03      0.04        60\n",
      "           5       0.08      0.09      0.09        54\n",
      "           6       0.13      0.11      0.12        46\n",
      "           7       0.10      0.14      0.12        44\n",
      "           8       0.17      0.16      0.16        51\n",
      "           9       0.09      0.08      0.08        49\n",
      "          10       0.07      0.04      0.05        45\n",
      "          11       0.05      0.04      0.04        51\n",
      "          12       0.08      0.15      0.10        41\n",
      "          13       0.06      0.09      0.07        47\n",
      "          14       0.08      0.07      0.08        43\n",
      "          15       0.14      0.15      0.14        48\n",
      "          16       0.06      0.05      0.05        42\n",
      "          17       0.07      0.06      0.06        50\n",
      "          18       0.13      0.10      0.11        49\n",
      "\n",
      "    accuracy                           0.09       915\n",
      "   macro avg       0.08      0.09      0.08       915\n",
      "weighted avg       0.08      0.09      0.08       915\n",
      "\n",
      "[[ 2  2  1  4  2  6  4 10  0  0  4  3  4  2  5  2  0  1  1]\n",
      " [ 1  0  4  5  4  5  2  1  3  4  2  0  3  1  3  4  2  4  1]\n",
      " [ 3  2  4  5  0  2  2  4  3  3  1  2  2  4  3  2  3  4  1]\n",
      " [ 2  1  1  8  0  4  1  1  3  3  2  2  5  2  1  0  2  3  2]\n",
      " [ 4  4  6  8  2  2  2  3  2  3  0  2  3  5  2  4  3  2  3]\n",
      " [ 1  1  3  1  2  5  3  1  3  4  3  3  5  4  4  2  3  3  3]\n",
      " [ 3  2  1  3  1  7  5  3  2  3  1  1  1  3  0  4  3  2  1]\n",
      " [ 0  2  1  2  2  3  1  6  5  2  1  1  4  4  3  1  2  2  2]\n",
      " [ 2  3  2  4  0  3  3  5  8  1  2  1  4  1  1  5  1  3  2]\n",
      " [ 5  1  3  3  0  4  1  2  2  4  2  1  8  3  0  2  2  1  5]\n",
      " [ 1  2  2  3  5  2  2  0  2  1  2  6  6  3  0  4  2  0  2]\n",
      " [ 1  5  2  5  1  1  1  5  6  3  1  2  3  6  1  1  0  2  5]\n",
      " [ 1  1  6  3  1  1  2  2  2  2  0  0  6  6  0  2  4  1  1]\n",
      " [ 4  1  6  3  2  3  0  1  0  2  0  5  4  4  2  2  2  4  2]\n",
      " [ 3  2  3  4  2  2  2  3  1  0  1  4  2  5  3  2  2  2  0]\n",
      " [ 3  0  2  4  3  2  1  5  0  3  2  3  2  4  2  7  2  3  0]\n",
      " [ 1  5  6  1  1  3  4  2  0  2  1  1  4  3  2  1  2  2  1]\n",
      " [ 3  1  7  2  1  4  2  4  4  2  2  2  5  1  2  2  1  3  2]\n",
      " [ 2  2  5  8  1  2  1  2  1  4  3  1  4  1  2  3  0  2  5]]\n"
     ]
    }
   ],
   "source": [
    "# param_grid = {\n",
    "#     'kernel_size': [3, 5], #!!\n",
    "#     'num_layers': [2, 3],\n",
    "#     'batch_size': [32, 64],\n",
    "#     'learning_rate': [0.001, 0.0001]\n",
    "# }\n",
    "# param_grid = {\n",
    "#     'kernel_size': [5],\n",
    "#     'num_layers': [3],\n",
    "#     'batch_size': [32],\n",
    "#     'learning_rate': [0.001]\n",
    "# }\n",
    "\n",
    "# model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "\n",
    "# grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# best_params = grid_result.best_params_\n",
    "\n",
    " \n",
    "best_params = {'batch_size': 32, 'kernel_size': 5, 'learning_rate': 0.001, 'num_layers': 3}\n",
    "final_model = create_model(**best_params)\n",
    "final_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
    "\n",
    "y_pred = final_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "print(confusion_matrix(y_test, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.save('model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detección de clase con una sola imágen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 372ms/step\n",
      "M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = load_model('model.h5')\n",
    "\n",
    "class_indices = json.load(open('class_indices.json'))\n",
    "\n",
    "def predict_image(model, img_path, class_indices):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "    prediction = model.predict(img)\n",
    "    predicted_class = list(class_indices.keys())[np.argmax(prediction)]\n",
    "    return predicted_class\n",
    "\n",
    "img_path = \"./classes_original/M/DSC01254.JPG\"\n",
    "predicted_class = predict_image(model, img_path, class_indices)\n",
    "print(predicted_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detección de clases en tiempo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quiero que me haga predicciones en tiempo real con la camara\n",
    "\n",
    "\n",
    "model = load_model('model.h5')\n",
    "\n",
    "class_indices = json.load(open('class_indices.json'))\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    img = cv2.resize(frame, (224, 224))\n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "    prediction = model.predict(img)\n",
    "    predicted_class = list(class_indices.keys())[np.argmax(prediction)]\n",
    "    cv2.putText(frame, predicted_class, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
